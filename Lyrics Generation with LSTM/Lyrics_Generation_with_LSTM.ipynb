{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyrics Generation with LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMtYJUE9dHhP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "outputId": "30003242-df3f-4279-8b26-35f6442aaed6"
      },
      "source": [
        "!pip install tensorflow==1.6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/0f/fbd8bb92459c75db93040f80702ebe4ba83a52cdb6ad930654c31dc0b711/tensorflow-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (45.8MB)\n",
            "\u001b[K     |████████████████████████████████| 45.9MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.18.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.28.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.1.0)\n",
            "Collecting tensorboard<1.7.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/67/a8c91665987d359211dcdca5c8b2a7c1e0876eb0702a4383c1e4ff76228d/tensorboard-1.6.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.6) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6) (3.2.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6) (1.0.1)\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=ff24caf37fd2492baebaa5d094b0774ca4cc6d77d8dfdf13c06d5903125ee8fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.4\n",
            "    Uninstalling bleach-3.1.4:\n",
            "      Successfully uninstalled bleach-3.1.4\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.6.0 tensorflow-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CE9lsuCdn7i",
        "colab_type": "text"
      },
      "source": [
        "# **Lyrics Generation wit LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8wzNwwdKTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "#print(tensorflow.__version__)\n",
        "from tensorflow.contrib import rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgh5DWUGdulC",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing Step**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KnrOl30dwI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d2a36f62-e6ba-4b9f-982f-afec0b67b88c"
      },
      "source": [
        "!pip install codecs\n",
        "import numpy as np\n",
        "import codecs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement codecs (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for codecs\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eon6IPLzd27X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocessing:\n",
        "  vocabulary = {}\n",
        "  binary_vocabulary = {}\n",
        "  char_lookup = {}\n",
        "  size = 0\n",
        "  separator = '->'\n",
        "  def generate(self,input_file_path):\n",
        "    input_file = codecs.open(input_file_path,'r','utf_8')\n",
        "    index = 0\n",
        "    for line in input_file:\n",
        "      for char in line:\n",
        "        if char not in self.vocabulary:\n",
        "          self.vocabulary[char] = index\n",
        "          self.char_lookup[index] = char\n",
        "          index +=1\n",
        "    input_file.close()\n",
        "    self.set_vocabulary_size()\n",
        "    self.create_binary_representation()\n",
        "  def retrieve(self,input_file_path):\n",
        "    input_file = codecs.open(input_file_path,'r','utf_8')\n",
        "    buffer = \"\"\n",
        "    for line in input_file:\n",
        "      try:\n",
        "        separator_position = len(buffer) + line.index(self.separator)\n",
        "        buffer += line\n",
        "        key = buffer[:separator_position]\n",
        "        value = buffer[separator_position + len(self.separator):]\n",
        "        value = np.fromstring(value,sep=',')\n",
        "        self.binary_vocabulary[key] = value\n",
        "        self.vocabulary[key] = np.where(value==1)[0][0]\n",
        "        self.char_lookup[np.where(value==1)[0][0]] = key\n",
        "        buffer = \"\"\n",
        "      except ValueError:\n",
        "        buffer +=line\n",
        "    input_file.close()\n",
        "    self.set_vocabulary_size()\n",
        "  def create_binary_representation(self):\n",
        "    for key,value in self.vocabulary.iteritems():\n",
        "      binary = np.zeros(self.size)\n",
        "      binary[value] = 1\n",
        "      self.binary_vocabulary[key] = binary\n",
        "  def set_vocabulary_size(self):\n",
        "    self.size = len(self.vocabulary)\n",
        "    print(\"Vocabulary Size: {}\".format(self.size))\n",
        "  def get_serialized_binary_representation(self):\n",
        "    string = \"\"\n",
        "    np.set_printoptions(threshold='nan')\n",
        "    for key,value in self.binary_vocabulary.iteritems():\n",
        "      array_as_string  = np.array2string(value,separator= ',',max_line_width = self.size * self.size)\n",
        "      string  +=\"{}{}{}\\n\".format(key.encode('utf-8'),self.separator,array_as_string[1:len(array_as_string) -1])\n",
        "    return string\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GfU3Xufd76e",
        "colab_type": "text"
      },
      "source": [
        "# **Defining Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkXbU__Vd0Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  x = None\n",
        "  y = None\n",
        "  sequence_length = None\n",
        "  weights = None\n",
        "  baises = None\n",
        "  outputs = None\n",
        "\n",
        "  def __init__(self,name):\n",
        "    self.name = name\n",
        "  \n",
        "  def build(self,input_number, sequence_length,layers_number,units_number,output_number):\n",
        "    self.x = tf.placeholder(\"float\",[None,sequence_length,input_number])\n",
        "    self.y = tf.placeholder(\"float\",[None,output_number])\n",
        "    self.sequence_length =sequence_length\n",
        "\n",
        "    self.weights = {\n",
        "        'out':tf.Variable(tf.random_normal([units_number,output_number]))\n",
        "    }\n",
        "    self.baises = {\n",
        "        'out': tf.Variable(tf.random_normal([output_number]))\n",
        "    }\n",
        "    x = tf.transpose(self.x,[1,0,2])\n",
        "    x = tf.reshape(x,[-1,input_number])\n",
        "    x = tf.split(x,sequence_length,0)\n",
        "    lstm_layers = []\n",
        "    for i in range(0,layers_number):\n",
        "      lstm_layer = rnn.BasicLSTMCell(units_number)\n",
        "      lstm_layer.append(lstm_layer)\n",
        "    deep_lstm = rnn.MultiRNNCell(lstm_layers)\n",
        "    self.outputs, states = rnn.static_rnn(deep_lstm,x,dtype = tf.float32)\n",
        "    print(\"Build model with input number:{},sequence_length:{},layers_number:{},\"\\\n",
        "    \" units_number:{},output_number:{}\".format(input_number,sequence_length,layers_number,units_number,output_number))\n",
        "    self.save(input_number,sequence_length,layers_number,units_number,output_number)  \n",
        "  def save(self,input_number, sequence_length,layers_number,units_number,output_number):\n",
        "    config = {\n",
        "        \"input_number\":input_number,\n",
        "        \"sequence_length\":sequence_length,\n",
        "        \"layers_number\":layers_number,\n",
        "        \"units_number\":units_number,\n",
        "        \"output_number\":output_number\n",
        "    }\n",
        "    config_file = open(self.get_config_file_path(),'w')\n",
        "    pickle.dump(config,config_file)\n",
        "    config_file.close()\n",
        "  def restore(self):\n",
        "    config_file = open(self.get_config_file_path(),'r')\n",
        "    config = pickle.load(config_file)\n",
        "    config_file.close()\n",
        "\n",
        "    self.build(config[\"input number\"],config[\"sequence length\"],config[\"layers_number\"],\n",
        "               config[\"units_number\"],config[\"output_number\"])\n",
        "  def get_classifier(self):\n",
        "    return tf.matmul(self.outputs[-1], self.weights['out']) + self.baises['out']\n",
        "  def get_config_file_path(self):\n",
        "    return \"{}/{}.config\".format(self.name,self.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjSIn6vykFGc",
        "colab_type": "text"
      },
      "source": [
        "# **Train Deep Tensorflow based LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmkUygc2kLJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import codecs\n",
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRn8D4fpkjut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "73bef852-b615-4fc5-a232-f33a32c55166"
      },
      "source": [
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--training_file', type=str, required=True)\n",
        "    parser.add_argument('--vocabulary_file', type=str, required=True)\n",
        "    parser.add_argument('--model_name', type=str, required=True)\n",
        "\n",
        "    parser.add_argument('--epoch', type=int, default=200)\n",
        "    parser.add_argument('--batch_size', type=int, default=50)\n",
        "    parser.add_argument('--sequence_length', type=int, default=50)\n",
        "    parser.add_argument('--log_frequency', type=int, default=100)\n",
        "    parser.add_argument('--learning_rate', type=int, default=0.002)\n",
        "    parser.add_argument('--units_number', type=int, default=128)\n",
        "    parser.add_argument('--layers_number', type=int, default=2)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    training_file = args.training_file\n",
        "    vocabulary_file = args.vocabulary_file\n",
        "    model_name = args.model_name\n",
        "\n",
        "    epoch = args.epoch\n",
        "    batch_size = args.batch_size\n",
        "    sequence_length = args.sequence_length\n",
        "    log_frequency = args.log_frequency\n",
        "    learning_rate = args.learning_rate\n",
        "\n",
        "    batch = Batch(training_file, vocabulary_file, batch_size, sequence_length)\n",
        "\n",
        "    input_number = batch.vocabulary.size\n",
        "    classes_number = batch.vocabulary.size\n",
        "    units_number = args.units_number\n",
        "    layers_number = args.layers_number\n",
        "\n",
        "    print(\"Start training with epoch: {}, batch_size: {}, log_frequency: {},\" \\\n",
        "          \"learning_rate: {}\".format(epoch, batch_size, log_frequency, learning_rate))\n",
        "\n",
        "    if not os.path.exists(model_name):\n",
        "        os.makedirs(model_name)\n",
        "\n",
        "    model = Model(model_name)\n",
        "    model.build(input_number, sequence_length, layers_number, units_number, classes_number)\n",
        "    classifier = model.get_classifier()\n",
        "\n",
        "    cost = tf.reduce_mean(tf.square(classifier - model.y))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "    expected_prediction = tf.equal(tf.argmax(classifier, 1), tf.argmax(model.y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(expected_prediction, tf.float32))\n",
        "    \n",
        "    loss_summary = tf.summary.scalar(\"loss\", cost)\n",
        "    acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
        "    \n",
        "    train_summary_op = tf.summary.merge_all()\n",
        "    out_dir = \"{}/{}\".format(model_name, model_name)\n",
        "    train_summary_dir = os.path.join(out_dir, \"summaries\")\n",
        "    \n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        iteration = 0\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir,sess.graph)\n",
        "        \n",
        "        while batch.dataset_full_passes < epoch:\n",
        "            iteration += 1\n",
        "            batch_x, batch_y = batch.get_next_batch()\n",
        "            batch_x = batch_x.reshape((batch_size, sequence_length, input_number))\n",
        "\n",
        "            sess.run(optimizer, feed_dict={model.x: batch_x, model.y: batch_y})\n",
        "            if iteration % log_frequency == 0:\n",
        "                acc,summaries = sess.run([accuracy,train_summary_op], feed_dict={model.x: batch_x, model.y: batch_y})\n",
        "                loss = sess.run(cost, feed_dict={model.x: batch_x, model.y: batch_y})\n",
        "                print(\"Iteration {}, batch loss: {:.6f}, training accuracy: {:.5f}\".format(iteration * batch_size,\n",
        "                                                                                           loss, acc))\n",
        "                train_summary_writer.add_summary(summaries, iteration * batch_size)\n",
        "        batch.clean()\n",
        "        print(\"Optimization done\")\n",
        "\n",
        "        saver = tf.train.Saver(tf.global_variables())\n",
        "        checkpoint_path = \"{}/{}.ckpt\".format(model_name, model_name)\n",
        "        saver.save(sess, checkpoint_path, global_step=iteration * batch_size)\n",
        "        print(\"Model saved in {}\".format(model_name))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] --training_file TRAINING_FILE\n",
            "                             --vocabulary_file VOCABULARY_FILE --model_name\n",
            "                             MODEL_NAME [--epoch EPOCH]\n",
            "                             [--batch_size BATCH_SIZE]\n",
            "                             [--sequence_length SEQUENCE_LENGTH]\n",
            "                             [--log_frequency LOG_FREQUENCY]\n",
            "                             [--learning_rate LEARNING_RATE]\n",
            "                             [--units_number UNITS_NUMBER]\n",
            "                             [--layers_number LAYERS_NUMBER]\n",
            "ipykernel_launcher.py: error: the following arguments are required: --training_file, --vocabulary_file, --model_name\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtin1v7gpYNl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "18e31520-c68c-401c-e44b-91382d8791db"
      },
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model_name', type=str, required=True)\n",
        "    parser.add_argument('--vocabulary_file', type=str, required=True)\n",
        "    parser.add_argument('--output_file', type=str, required=True)\n",
        "\n",
        "    parser.add_argument('--seed', type=str, default=\"Yeah, oho \")\n",
        "    parser.add_argument('--sample_length', type=int, default=1500)\n",
        "    parser.add_argument('--log_frequency', type=int, default=100)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    model_name = args.model_name\n",
        "    vocabulary_file = args.vocabulary_file\n",
        "    output_file = args.output_file\n",
        "    seed = args.seed.decode('utf-8')\n",
        "    sample_length = args.sample_length\n",
        "    log_frequency = args.log_frequency\n",
        "\n",
        "    model = Model(model_name)\n",
        "    model.restore()\n",
        "    classifier = model.get_classifier()\n",
        "\n",
        "    vocabulary = Preprocessing()\n",
        "    vocabulary.retrieve(vocabulary_file)\n",
        "\n",
        "    sample_file = codecs.open(output_file, 'w', 'utf_8')\n",
        "\n",
        "    stack = deque([])\n",
        "    for i in range(0, model.sequence_length - len(seed)):\n",
        "        stack.append(u' ')\n",
        "\n",
        "    for char in seed:\n",
        "        if char not in vocabulary.vocabulary:\n",
        "            print(char,\"is not in vocabulary file\")\n",
        "            char = u' '\n",
        "        stack.append(char)\n",
        "        sample_file.write(char)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "        saver = tf.train.Saver(tf.global_variables())\n",
        "        ckpt = tf.train.get_checkpoint_state(model_name)\n",
        "\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "            for i in range(0, sample_length):\n",
        "                vector = []\n",
        "                for char in stack:\n",
        "                    vector.append(vocabulary.binary_vocabulary[char])\n",
        "                vector = np.array([vector])\n",
        "                prediction = sess.run(classifier, feed_dict={model.x: vector})\n",
        "                predicted_char = vocabulary.char_lookup[np.argmax(prediction)]\n",
        "\n",
        "                stack.popleft()\n",
        "                stack.append(predicted_char)\n",
        "                sample_file.write(predicted_char)\n",
        "\n",
        "                if i % log_frequency == 0:\n",
        "                    print(\"Progress: {}%\".format((i * 100) / sample_length))\n",
        "\n",
        "            sample_file.close()\n",
        "            print(\"Lyrics saved in {}\".format(output_file))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] --model_name MODEL_NAME --vocabulary_file\n",
            "                             VOCABULARY_FILE --output_file OUTPUT_FILE\n",
            "                             [--seed SEED] [--sample_length SAMPLE_LENGTH]\n",
            "                             [--log_frequency LOG_FREQUENCY]\n",
            "ipykernel_launcher.py: error: the following arguments are required: --model_name, --vocabulary_file, --output_file\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2judxRPq_pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}