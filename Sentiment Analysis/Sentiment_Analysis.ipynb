{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7fvdnvtF1bO",
        "colab_type": "code",
        "outputId": "a826753b-48a3-4aae-854d-2a51c9b053d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        }
      },
      "source": [
        "!pip install tensorflow==1.6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/0f/fbd8bb92459c75db93040f80702ebe4ba83a52cdb6ad930654c31dc0b711/tensorflow-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (45.8MB)\n",
            "\u001b[K     |████████████████████████████████| 45.9MB 68kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.12.0)\n",
            "Collecting tensorboard<1.7.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/67/a8c91665987d359211dcdca5c8b2a7c1e0876eb0702a4383c1e4ff76228d/tensorboard-1.6.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.28.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.6) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.6) (46.1.3)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow==1.6) (3.2.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107220 sha256=10420557e05b5efbe89e96d2d8e58b5c7d090d96fde5dc7976821a8d5cf10f5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.4\n",
            "    Uninstalling bleach-3.1.4:\n",
            "      Successfully uninstalled bleach-3.1.4\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.6.0 tensorflow-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrSSgMs9Vcvg",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Analysis Using RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOupODYpTjMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.datasets\n",
        "import numpy as np\n",
        "import re\n",
        "import collections\n",
        "from sklearn import metrics\n",
        "\n",
        "def clearstring(string):\n",
        "    string = re.sub('[^A-Za-z0-9 ]+', '', string)\n",
        "    string = string.split(' ')\n",
        "    string = filter(None, string)\n",
        "    string = [y.strip() for y in string]\n",
        "    string = ' '.join(string)\n",
        "    return string.lower()\n",
        "\n",
        "def separate_dataset(trainset,ratio=0.5):\n",
        "    datastring = []\n",
        "    datatarget = []\n",
        "    for i in range(int(len(trainset.data)*ratio)):\n",
        "        data_ = trainset.data[i].split('\\n')\n",
        "        data_ = list(filter(None, data_))\n",
        "        for n in range(len(data_)):\n",
        "            data_[n] = clearstring(data_[n])\n",
        "        datastring += data_\n",
        "        for n in range(len(data_)):\n",
        "            datatarget.append(trainset.target[i])\n",
        "    return datastring, datatarget\n",
        "\n",
        "def build_dataset(words, n_words):\n",
        "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        index = dictionary.get(word, 0)\n",
        "        if index == 0:\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary\n",
        "\n",
        "def str_idx(corpus, dic, maxlen, UNK=3):\n",
        "    X = np.zeros((len(corpus),maxlen))\n",
        "    for i in range(len(corpus)):\n",
        "        for no, k in enumerate(corpus[i].split()[:maxlen][::-1]):\n",
        "            try:\n",
        "                X[i,-1 - no]=dic[k]\n",
        "            except Exception as e:\n",
        "                X[i,-1 - no]=UNK\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxTgixmpG4KV",
        "colab_type": "code",
        "outputId": "9f79752a-258b-490b-a5d9-68bc40a19430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 451133319805618703]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6kFzRW2Pzqg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "45b51d15-51a9-4233-c59f-19989b08a3ee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx3YT2bJRjMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c97c7243-4abe-4efb-fc84-df68b84276b4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow\n",
        "import sklearn\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kf78CYeSXjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJGmB-eqSlub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time,os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaooYpEhSnTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = sklearn.datasets.load_files(container_path = '/content/drive/My Drive/sentiment_data',encoding = 'UTF-8')\n",
        "trainset.data, trainset.target = separate_dataset(trainset,1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1kEhBpgT1ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "646ac1f3-20aa-464f-84b4-ec1084311960"
      },
      "source": [
        "print(trainset.target_names)\n",
        "print(len(trainset.data))\n",
        "print(len(trainset.target))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative', 'positive']\n",
            "10662\n",
            "10662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbrLH5lZSwom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ONEHOT = np.zeros((len(trainset.data),len(trainset.target_names)))\n",
        "ONEHOT[np.arange(len(trainset.data)),trainset.target] = 1.0\n",
        "train_x, test_x, train_y, test_y, train_onehot,test_onehot = train_test_split(trainset.data, trainset.target,ONEHOT,test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfJKUb60S9nr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "51bfaa14-7729-4159-b03c-57bcda239527"
      },
      "source": [
        "concat = ' '.join(trainset.data).split()\n",
        "vocabulary_size = len(list(set(concat)))\n",
        "data, count, dictionary, rev_dictionary= build_dataset(concat, vocabulary_size)\n",
        "print('vocab from size: %d'%(vocabulary_size))\n",
        "print('Most common words', count[4:10])\n",
        "print('Sample data ',data[:10], [rev_dictionary[i] for i in data[:10]])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab from size: 20465\n",
            "Most common words [('the', 10129), ('a', 7312), ('and', 6199), ('of', 6063), ('to', 4233), ('is', 3378)]\n",
            "Sample data  [4, 645, 9, 2540, 8, 22, 4, 3242, 10286, 97] ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'centurys', 'new']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJRwBih6T_YV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GO = dictionary['GO']\n",
        "PAD = dictionary['PAD']\n",
        "EOS = dictionary['EOS']\n",
        "UNK = dictionary['UNK']\n",
        "size_layer = 128\n",
        "num_layers = 2\n",
        "embedded_size = 128\n",
        "dimension_output = len(trainset.target_names)\n",
        "learning_rate = 1e-3\n",
        "maxlen = 50\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW0yHdf5UDDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, size_layer, num_layers, embedded_size,\n",
        "                 dict_size, dimension_output, learning_rate):\n",
        "        \n",
        "        def cells(reuse=False):\n",
        "            return tf.nn.rnn_cell.BasicRNNCell(size_layer,reuse=reuse)\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, None])\n",
        "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
        "        \n",
        "        encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size], -1, 1))\n",
        "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
        "        \n",
        "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
        "        outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype = tf.float32)\n",
        "        \n",
        "        W = tf.get_variable('w',shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())\n",
        "        b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())\n",
        "        \n",
        "        \n",
        "        self.logits = tf.matmul(outputs[:, -1], W) + b\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "        \n",
        "        correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXrer7plUF86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "2882baea-cb38-4a1e-b267-a3f7f16e7047"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "model = Model(size_layer,num_layers,embedded_size,vocabulary_size+4,dimension_output,learning_rate)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-71c8c7524c64>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uo69bdEUTt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\n",
        "checkpoint_dir = os.path.abspath(os.path.join('./', \"checkpoints_basic_rnn\"))\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPawJXr2UZPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "2cc481ec-6754-4113-95e7-af4dcc628663"
      },
      "source": [
        "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0\n",
        "while True:\n",
        "    lasttime = time.time()\n",
        "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
        "        print('break epoch:%d\\n'%(EPOCH))\n",
        "        break\n",
        "        \n",
        "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
        "    for i in range(0, (len(train_x) // batch_size) * batch_size, batch_size):\n",
        "        batch_x = str_idx(train_x[i:i+batch_size],dictionary,maxlen)\n",
        "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
        "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
        "        train_loss += loss\n",
        "        train_acc += acc\n",
        "    \n",
        "    for i in range(0, (len(test_x) // batch_size) * batch_size, batch_size):\n",
        "        batch_x = str_idx(test_x[i:i+batch_size],dictionary,maxlen)\n",
        "        acc, loss = sess.run([model.accuracy, model.cost], \n",
        "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
        "        test_loss += loss\n",
        "        test_acc += acc\n",
        "    \n",
        "    train_loss /= (len(train_x) // batch_size)\n",
        "    train_acc /= (len(train_x) // batch_size)\n",
        "    test_loss /= (len(test_x) // batch_size)\n",
        "    test_acc /= (len(test_x) // batch_size)\n",
        "    \n",
        "    if test_acc > CURRENT_ACC:\n",
        "        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n",
        "        CURRENT_ACC = test_acc\n",
        "        CURRENT_CHECKPOINT = 0\n",
        "    else:\n",
        "        CURRENT_CHECKPOINT += 1\n",
        "        \n",
        "    print('time taken:', time.time()-lasttime)\n",
        "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n",
        "                                                                                          train_acc,test_loss,\n",
        "                                                                                          test_acc))\n",
        "    path = saver.save(sess, checkpoint_prefix, global_step=EPOCH)\n",
        "    EPOCH += 1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, pass acc: 0.000000, current acc: 0.501465\n",
            "time taken: 15.299572467803955\n",
            "epoch: 0, training loss: 0.706370, training acc: 0.535866, valid loss: 0.739801, valid acc: 0.501465\n",
            "\n",
            "time taken: 14.949910402297974\n",
            "epoch: 1, training loss: 0.619355, training acc: 0.661458, valid loss: 0.824721, valid acc: 0.499512\n",
            "\n",
            "time taken: 14.770671367645264\n",
            "epoch: 2, training loss: 0.505822, training acc: 0.754261, valid loss: 1.012179, valid acc: 0.492676\n",
            "\n",
            "time taken: 15.318742036819458\n",
            "epoch: 3, training loss: 0.361009, training acc: 0.843868, valid loss: 1.264383, valid acc: 0.499023\n",
            "\n",
            "time taken: 14.937932968139648\n",
            "epoch: 4, training loss: 0.236770, training acc: 0.905658, valid loss: 1.780690, valid acc: 0.499023\n",
            "\n",
            "time taken: 14.862792253494263\n",
            "epoch: 5, training loss: 0.152105, training acc: 0.943774, valid loss: 2.107150, valid acc: 0.489258\n",
            "\n",
            "break epoch:6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al1pQWQAUdaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "485787c8-fb8d-4d4e-8532-c4ce114843f2"
      },
      "source": [
        "logits = sess.run(model.logits, feed_dict={model.X:str_idx(test_x,dictionary,maxlen)})\n",
        "print(metrics.classification_report(test_y, np.argmax(logits,1), target_names = trainset.target_names))\n",
        "\n",
        "\n",
        "\n",
        "#Predict\n",
        "checkpoint_file = tf.train.latest_checkpoint(os.path.join('./', 'checkpoints_basic_rnn'))\n",
        "saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "saver.restore(sess, checkpoint_file)\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "    logits = sess.run(model.logits, feed_dict={model.X:str_idx([sentence],dictionary,maxlen)})\n",
        "    return trainset.target_names[np.argmax(logits,1)[0]]\n",
        "\n",
        "\n",
        "predict('i love this book')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.62      0.73      0.67      1026\n",
            "    positive       0.70      0.58      0.63      1107\n",
            "\n",
            "    accuracy                           0.65      2133\n",
            "   macro avg       0.66      0.65      0.65      2133\n",
            "weighted avg       0.66      0.65      0.65      2133\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/checkpoints_basic_rnn/model-5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMG9I0hWYomo",
        "colab_type": "text"
      },
      "source": [
        "# **RNN and LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6WWCXW9Yr0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXB9cCiJYzgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ac9ce144-2a58-4b5c-e2a9-1d7f4b73db4d"
      },
      "source": [
        "trainset = sklearn.datasets.load_files(container_path='/content/drive/My Drive/sentiment_data',encoding='UTF-8')\n",
        "trainset.data, trainset.target = separate_dataset(trainset,1.0)\n",
        "print(trainset.target_names)\n",
        "print(len(trainset.data))\n",
        "print(len(trainset.target))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative', 'positive']\n",
            "10662\n",
            "10662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW2GzWe8ZhRY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d103b7d0-a0d5-4ebf-8e72-a913c436a715"
      },
      "source": [
        "ONEHOT = np.zeros((len(trainset.data),len(trainset.target_names)))\n",
        "ONEHOT[np.arange(len(trainset.data)),trainset.target] = 1.0\n",
        "train_X, test_X, train_Y, test_Y , train_onehot, test_onehot = train_test_split(trainset.data,trainset.target, ONEHOT,test_size=0.2)\n",
        "\n",
        "concat = ' '.join(trainset.data).split()\n",
        "vocabulary_size = len(list(set(concat)))\n",
        "data, count, dictionary, rev_dictionary = build_dataset(concat, vocabulary_size)\n",
        "print('Vocab from size: %d'%(vocabulary_size))\n",
        "print('Most Common words',count[4:10])\n",
        "print('Sample data', data[:10],[rev_dictionary[i] for i in data[:10]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab from size: 20465\n",
            "Most Common words [('the', 10129), ('a', 7312), ('and', 6199), ('of', 6063), ('to', 4233), ('is', 3378)]\n",
            "Sample data [4, 645, 9, 2540, 8, 22, 4, 3242, 10286, 97] ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'centurys', 'new']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1t0Oh4Taqml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GO = dictionary['GO']\n",
        "PAD = dictionary['PAD']\n",
        "EOS = dictionary['EOS']\n",
        "UNK = dictionary['UNK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIfDbFAWa-nQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_layer = 128\n",
        "num_layers = 2\n",
        "embedded_size = 128\n",
        "dimension_output = len(trainset.target_names)\n",
        "learning_rate = 1e-3\n",
        "maxlen = 50\n",
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEIpgm_kbROJ",
        "colab_type": "text"
      },
      "source": [
        "# **Define LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY4ZO5i7bVCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "  def __init__(self,size_layer,num_layers,embedded_size, dict_size, dimension_output, learning_rate):\n",
        "    def cells(reuse=False):\n",
        "      return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
        "    self.X = tf.placeholder(tf.int32, [None,None])\n",
        "    self.Y = tf.placeholder(tf.float32,[None,dimension_output])\n",
        "    encoder_embeddings = tf.Variable(tf.random_uniform([dict_size, embedded_size],-1,1))\n",
        "    encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
        "    rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
        "    outputs, _ = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded, dtype =tf.float32)\n",
        "    W =tf.get_variable('w', shape=(size_layer, dimension_output),initializer=tf.orthogonal_initializer())\n",
        "    b = tf.get_variable('b',shape=(dimension_output),initializer=tf.zeros_initializer())\n",
        "    self.logits = tf.matmul(outputs[:,-1],W)+ b\n",
        "    self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,labels=self.Y))\n",
        "    self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
        "    correct_pred = tf.equal(tf.argmax(self.logits,1),tf.argmax(self.Y,1))\n",
        "    self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qymDLhm3d5Nx",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOLmi-RUd7nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "model = Model(size_layer, num_layers, embedded_size, vocabulary_size+4, dimension_output, learning_rate)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver(tf.global_variables(),max_to_keep=2)\n",
        "checkpoint_dir = os.path.abspath(os.path.join('./',\"checkpoint_rnn_lstms\"))\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjOcG3JseqF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "outputId": "9ce2345c-dc6d-4305-a46a-a6e91dfb7473"
      },
      "source": [
        "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 5, 0, 0, 0\n",
        "while True:\n",
        "    lasttime = time.time()\n",
        "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
        "        print('break epoch:%d\\n'%(EPOCH))\n",
        "        break\n",
        "        \n",
        "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
        "    for i in range(0, (len(train_X) // batch_size) * batch_size, batch_size):\n",
        "        batch_x = str_idx(train_X[i:i+batch_size],dictionary,maxlen)\n",
        "        acc, loss, _ = sess.run([model.accuracy, model.cost, model.optimizer], \n",
        "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
        "        train_loss += loss\n",
        "        train_acc += acc\n",
        "    \n",
        "    for i in range(0, (len(test_X) // batch_size) * batch_size, batch_size):\n",
        "        batch_x = str_idx(test_X[i:i+batch_size],dictionary,maxlen)\n",
        "        acc, loss = sess.run([model.accuracy, model.cost], \n",
        "                           feed_dict = {model.X : batch_x, model.Y : train_onehot[i:i+batch_size]})\n",
        "        test_loss += loss\n",
        "        test_acc += acc\n",
        "    \n",
        "    train_loss /= (len(train_X) // batch_size)\n",
        "    train_acc /= (len(train_X) // batch_size)\n",
        "    test_loss /= (len(test_X) // batch_size)\n",
        "    test_acc /= (len(test_X) // batch_size)\n",
        "    \n",
        "    if test_acc > CURRENT_ACC:\n",
        "        print('epoch: %d, pass acc: %f, current acc: %f'%(EPOCH,CURRENT_ACC, test_acc))\n",
        "        CURRENT_ACC = test_acc\n",
        "        CURRENT_CHECKPOINT = 0\n",
        "    else:\n",
        "        CURRENT_CHECKPOINT += 1\n",
        "        \n",
        "    print('time taken:', time.time()-lasttime)\n",
        "    print('epoch: %d, training loss: %f, training acc: %f, valid loss: %f, valid acc: %f\\n'%(EPOCH,train_loss,\n",
        "                                                                                          train_acc,test_loss,\n",
        "                                                                                          test_acc))\n",
        "    path = saver.save(sess, checkpoint_prefix, global_step=EPOCH)\n",
        "    \n",
        "    EPOCH += 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, pass acc: 0.000000, current acc: 0.495605\n",
            "time taken: 35.01903533935547\n",
            "epoch: 0, training loss: 0.670577, training acc: 0.573864, valid loss: 0.758578, valid acc: 0.495605\n",
            "\n",
            "epoch: 1, pass acc: 0.495605, current acc: 0.500000\n",
            "time taken: 34.75499868392944\n",
            "epoch: 1, training loss: 0.571715, training acc: 0.696733, valid loss: 0.917919, valid acc: 0.500000\n",
            "\n",
            "epoch: 2, pass acc: 0.500000, current acc: 0.501465\n",
            "time taken: 34.7116003036499\n",
            "epoch: 2, training loss: 0.428541, training acc: 0.799361, valid loss: 1.226348, valid acc: 0.501465\n",
            "\n",
            "time taken: 34.498682498931885\n",
            "epoch: 3, training loss: 0.283641, training acc: 0.880563, valid loss: 1.530149, valid acc: 0.496582\n",
            "\n",
            "epoch: 4, pass acc: 0.501465, current acc: 0.503906\n",
            "time taken: 34.18601417541504\n",
            "epoch: 4, training loss: 0.183671, training acc: 0.930398, valid loss: 1.928464, valid acc: 0.503906\n",
            "\n",
            "time taken: 34.94509530067444\n",
            "epoch: 5, training loss: 0.112126, training acc: 0.959399, valid loss: 2.385956, valid acc: 0.497559\n",
            "\n",
            "time taken: 34.78568410873413\n",
            "epoch: 6, training loss: 0.070802, training acc: 0.974905, valid loss: 3.426880, valid acc: 0.487793\n",
            "\n",
            "time taken: 34.696656465530396\n",
            "epoch: 7, training loss: 0.041320, training acc: 0.985559, valid loss: 3.693021, valid acc: 0.499023\n",
            "\n",
            "time taken: 34.03629016876221\n",
            "epoch: 8, training loss: 0.017484, training acc: 0.993963, valid loss: 4.056795, valid acc: 0.496094\n",
            "\n",
            "time taken: 33.94508504867554\n",
            "epoch: 9, training loss: 0.012212, training acc: 0.995739, valid loss: 4.058840, valid acc: 0.488281\n",
            "\n",
            "break epoch:10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLLmwZ4oiYNM",
        "colab_type": "text"
      },
      "source": [
        "# **Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFjo8k8Hicac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "b8cb0fd8-acb4-4a8b-aed0-86dfa263ae98"
      },
      "source": [
        "logits = sess.run(model.logits, feed_dict = {model.X: str_idx(test_X, dictionary,maxlen)})\n",
        "print(metrics.classification_report(test_Y, np.argmax(logits,1),target_names=trainset.target_names))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.70      0.70      0.70      1030\n",
            "    positive       0.72      0.73      0.72      1103\n",
            "\n",
            "    accuracy                           0.71      2133\n",
            "   macro avg       0.71      0.71      0.71      2133\n",
            "weighted avg       0.71      0.71      0.71      2133\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBFwxwkPjTgB",
        "colab_type": "text"
      },
      "source": [
        "# **Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MbfBQo_jWAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "2a96e820-fca5-43eb-db11-f0828c9c8996"
      },
      "source": [
        "checkpoint_file = tf.train.latest_checkpoint(os.path.join('./', 'checkpoints_rnn_lstm'))\n",
        "saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "saver.restore(sess, checkpoint_file)\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "    logits = sess.run(model.logits, feed_dict={model.X:str_idx([sentence],dictionary,maxlen)})\n",
        "    return trainset.target_names[np.argmax(logits,1)[0]]\n",
        "\n",
        "\n",
        "predict('i love this book')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c7a35b27d8ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoints_rnn_lstm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}.meta\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1901\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    626\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File None.meta does not exist."
          ]
        }
      ]
    }
  ]
}